{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.stats\n",
    "\n",
    "from textblob import TextBlob, Word, Blobber\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "from textblob.taggers import NLTKTagger\n",
    "from textblob import Word\n",
    "from textblob.base import BaseTokenizer\n",
    "from textblob import TextBlob\n",
    "from textblob import Blobber\n",
    "from textblob.tokenizers import WordTokenizer\n",
    "\n",
    "from numpy import isnan\n",
    "from nltk.corpus import stopwords\n",
    "data=pd.read_csv(\"Assignment 3 Edmunds Posts.csv\", usecols=[0])\n",
    "\n",
    "data['Posts'] = data['Posts'].map(lambda x: x.replace(\"LexusES\",\"ES\"))\n",
    "data['Posts'] = data['Posts'].map(lambda x: x.replace(\"ES330\",\"ES\"))\n",
    "data['Posts'] = data['Posts'].map(lambda x: x.replace(\"LS460\",\"LS\"))\n",
    "data['Posts'] = data['Posts'].map(lambda x: x.replace(\"LS470\",\"LS\"))\n",
    "data['Posts'] = data['Posts'].map(lambda x: x.replace(\"LexusLS\",\"LS\"))\n",
    "data['Posts'] = data['Posts'].map(lambda x: x.replace(\"LexusRX\",\"RX\"))\n",
    "\n",
    "sw=stopwords.words('english')\n",
    "\n",
    "sw.remove('not')\n",
    "\n",
    "models_skinny=[\"es\",\"ls\",\"rx\",\"a8\",\"a6\",\"3series\",\"5series\",\"7series\",\"xj\",\"sclass\"] \n",
    "\n",
    "def tokenizer (xstring):\n",
    "    global sw\n",
    "    tokens=list(TextBlob(str(xstring)).words)\n",
    "    removeStopWords=[word for word in tokens if word.lower() not in sw]\n",
    "    lemmaed=[Word(w).lemmatize() for w in removeStopWords]\n",
    "    lowercase=[word.lower() for word in lemmaed]\n",
    "    return lowercase\n",
    "\n",
    "def finder (tokensList, modelList, numberWords):\n",
    "    blanklist=[]\n",
    "    for i in xrange(len(tokensList)):\n",
    "        if tokensList[i] in modelList:\n",
    "            blanklist.append(tokensList[i-numberWords:i+1+numberWords])\n",
    "    return blanklist\n",
    "    \n",
    "def compiler(postseries,modelsList,numberofwords):\n",
    "    tb=Blobber(tokenizer=WordTokenizer())\n",
    "    \n",
    "    newseries=postseries.apply(tokenizer)\n",
    "    newseries=newseries.apply(lambda x: finder(x,modelsList, numberofwords))\n",
    "    newseries=newseries.apply(lambda x: [' '.join(inner) for inner in x])\n",
    "    \n",
    "    models=[\" \"+x+\" \" for x in modelsList]\n",
    "    df=pd.DataFrame(columns=modelsList).join(newseries, how=\"outer\")\n",
    "    \n",
    "    for index,l in enumerate(newseries):\n",
    "        for string in l: \n",
    "                for model, model_skinny in zip(models,modelsList): #loop over all the models in model list\n",
    "                    if model in string: #check if model is in a particular list\n",
    "                        if isnan(df[model_skinny].iloc[index]): #correcting for neutral\n",
    "                            df[model_skinny].iloc[index]=0\n",
    "                        df[model_skinny].iloc[index]+=tb(string).sentiment[0]  \n",
    "    return df\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "rsentiment1 = compiler(data['Posts'],models_skinny,2)\n",
    "rsentiment = rsentiment1.drop(['Posts'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testing = ['ES', 'LS', 'RX', 'A8', 'A6', '3series', '5series', '7series', 'XJ', 'Sclass']\n",
    "rsentiment.columns = testing \n",
    "rsentiment = rsentiment * 5\n",
    "\n",
    "\n",
    "#names = ['ES-LS','ES-RX','ES-A8','ES-A6','ES-3series','ES-5series','ES-7series','ES-XJ','ES-Sclass','LS-RX','LS-A8','LS-A6','LS-3series','LS-5series','LS-7series','LS-XJ','LS-Sclass','RX-A8','RX-A6','RX-3series','RX-5series','RX-7series','RX-XJ','RX-Sclass','A8-A6','A8-3series','A8-5series','A8-7series','A8-XJ','A8-Sclass','A6-3series','A6-5series','A6-7series','A6-XJ','A6-Sclass','3series-5series','3series-7series','3series-XJ','3series-Sclass','5series-7series','5series-XJ','5series-Sclass','7series-XJ','7series-Sclass','XJ-Sclass']\n",
    "names = list(rsentiment.columns)\n",
    "#i=1\n",
    "#j=1\n",
    "\n",
    "##Create the sentiment difference dataframe (ES-LS, ES-RX, etc)\n",
    "length_senti = len(rsentiment)\n",
    "width_senti = len(rsentiment.T)\n",
    "comparisons = pd.DataFrame(index=np.array(range(length_senti)))\n",
    "#Vectors for future use in the nodes\n",
    "node_name_1 = []\n",
    "node_name_2 = []\n",
    "\n",
    "n=1 #For counter\n",
    "for i in range(width_senti-1):\n",
    "    for j in range(width_senti - n):\n",
    "        comparisons[str(names[i]) + '-' + str(names[(j+n)])] = np.array(rsentiment[[i]]) - np.array(rsentiment[[j+n]])\n",
    "        node_name_1.append(names[i])\n",
    "        node_name_2.append(names[(j+n)])\n",
    "    n=n+1\n",
    "\n",
    "length_comp = len(comparisons.T)\n",
    "\n",
    "averages_df = pd.DataFrame(index=comparisons.columns, columns = ['Positive','Negative'])\n",
    "\n",
    "\n",
    "\n",
    "for i in comparisons.columns:\n",
    "    pos_sum = 0\n",
    "    neg_sum = 0    \n",
    "    pos_sum = [pos_sum + x for x in comparisons[i] if x > 0]\n",
    "    neg_sum = [neg_sum + x for x in comparisons[i] if x < 0]\n",
    "    if len(pos_sum) != 0:    \n",
    "        averages_df.loc[i,'Positive'] = sum(pos_sum)/len(pos_sum)\n",
    "    if len(neg_sum) != 0:\n",
    "        averages_df.loc[i,'Negative'] = sum(neg_sum)/len(neg_sum)\n",
    "\n",
    "averages_df['Node 1'] = node_name_1\n",
    "averages_df['Node 2'] = node_name_2\n",
    "\n",
    "##Creating the network graph\n",
    "G=nx.DiGraph()\n",
    "\n",
    "#Creating the labels for the nodes and adding a node to the G object\n",
    "labels = {}\n",
    "opp_labels = {}\n",
    "for i, value in enumerate(names):\n",
    "    labels[i] = str(value)\n",
    "    opp_labels[value] = i\n",
    "    G.add_node(i) #Adding the nodes\n",
    "    \n",
    "\n",
    "#Create the list of tuples for edges\n",
    "node_list = []\n",
    "for index, pref in enumerate(averages_df.loc[:,'Positive']): #Positives\n",
    "    if ~np.isnan(pref):\n",
    "        node_list.append((pd.Series(averages_df.loc[:,'Node 2'])[index],pd.Series(averages_df.loc[:,'Node 1'])[index]))\n",
    "        G.add_edge(*(opp_labels[pd.Series(averages_df.loc[:,'Node 2'])[index]],opp_labels[pd.Series(averages_df.loc[:,'Node 1'])[index]]))\n",
    "for index, pref in enumerate(averages_df.loc[:,'Negative']): #Negatives\n",
    "    if ~np.isnan(pref):\n",
    "        node_list.append((pd.Series(averages_df.loc[:,'Node 1'])[index],pd.Series(averages_df.loc[:,'Node 2'])[index]))\n",
    "        G.add_edge(*(opp_labels[pd.Series(averages_df.loc[:,'Node 1'])[index]],opp_labels[pd.Series(averages_df.loc[:,'Node 2'])[index]]))\n",
    "\n",
    "\n",
    "pos = nx.circular_layout(G) #Positions\n",
    "\n",
    "#nx.edges([(1,2),(1,0)])#node_list)\n",
    "\n",
    "nx.draw_networkx_labels(G,pos, labels, font_size=16)\n",
    "#nx.draw_networkx_nodes(G,pos,node_color='skyblue')\n",
    "nx.draw(G,pos,node_size=1000)\n",
    "###nx.Graph(node_list)\n",
    "\n",
    "fig1 = plt.gcf()\n",
    "plt.figure(num=None, figsize=(30,20))\n",
    "#plt.savefig('network.png', dpi=100)\n",
    "\n",
    "plt.show()\n",
    "fig1.savefig('network.png', dpi=1000)\n",
    "\n",
    "\n",
    "##Coding pagerank ------- Weighted\n",
    "\n",
    "\n",
    "#Weights out\n",
    "wout_dict = {}\n",
    "for index, value in enumerate(averages_df.ix[:,'Positive']):\n",
    "    if ~np.isnan(value):\n",
    "        if averages_df.ix[index,'Node 2'] in wout_dict:\n",
    "            wout_dict[averages_df.ix[index,'Node 2']] = wout_dict[averages_df.ix[index,'Node 2']] + 1\n",
    "        else:\n",
    "            wout_dict[averages_df.ix[index,'Node 2']] = 1\n",
    "for index, value in enumerate(averages_df.ix[:,'Negative']):\n",
    "    if ~np.isnan(value):\n",
    "        if averages_df.ix[index,'Node 1'] in wout_dict:\n",
    "            wout_dict[averages_df.ix[index,'Node 1']] = wout_dict[averages_df.ix[index,'Node 1']] + 1\n",
    "        else:\n",
    "            wout_dict[averages_df.ix[index,'Node 1']] = 1\n",
    "\n",
    "#Weights in\n",
    "win_dict = {}\n",
    "for index, value in enumerate(averages_df.ix[:,'Positive']):\n",
    "    if ~np.isnan(value):\n",
    "        if averages_df.ix[index,'Node 1'] in win_dict:\n",
    "            win_dict[averages_df.ix[index,'Node 1']] = win_dict[averages_df.ix[index,'Node 1']] + 1\n",
    "        else:\n",
    "            win_dict[averages_df.ix[index,'Node 1']] = 1\n",
    "for index, value in enumerate(averages_df.ix[:,'Negative']):\n",
    "    if ~np.isnan(value):\n",
    "        if averages_df.ix[index,'Node 2'] in win_dict:\n",
    "            win_dict[averages_df.ix[index,'Node 2']] = win_dict[averages_df.ix[index,'Node 2']] + 1\n",
    "        else:\n",
    "            win_dict[averages_df.ix[index,'Node 2']] = 1\n",
    "\n",
    "#Counting the number of outputs for each node\n",
    "counts_dict = {}\n",
    "#Create a dict of price data\n",
    "price_dict = {'A6':20000, 'A8':12000, '3series':220000, '5series':60000, '7series':14000, 'XJ':6600, 'ES':135000, 'LS':30000, 'RX':120000, 'Sclass':25000}\n",
    "\n",
    "\n",
    "price_list = pd.Series(name='Price')\n",
    "for dictkey in price_dict:\n",
    "    price_list[dictkey] = price_dict[dictkey]\n",
    "\n",
    "for index, value in enumerate(averages_df.ix[:,'Negative']):\n",
    "    if np.isnan(value):\n",
    "        continue\n",
    "    else:\n",
    "        if averages_df.ix[index,'Node 1'] in counts_dict:\n",
    "            counts_dict[averages_df.ix[index,'Node 1']] += 1\n",
    "        else:\n",
    "            counts_dict[averages_df.ix[index,'Node 1']] = 1\n",
    "\n",
    "\n",
    "for index, value in enumerate(averages_df.ix[:,'Positive']):\n",
    "    if np.isnan(value):\n",
    "        continue\n",
    "    else:\n",
    "        if averages_df.ix[index,'Node 2'] in counts_dict:\n",
    "            counts_dict[averages_df.ix[index,'Node 2']] += 1\n",
    "        else:\n",
    "            counts_dict[averages_df.ix[index,'Node 2']] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remaining Tasks\n",
    "\n",
    "Creating the matrix\n",
    "weighting the values\n",
    "Find Eigenvector/PageRank\n",
    "Coding Unweighted pagerank\n",
    "Counting the number of outputs for each node\n",
    "Creating the matrix\n",
    "Find Eigenvector/PageRank\n",
    "Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
